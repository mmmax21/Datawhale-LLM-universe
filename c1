1. 举例大模型和相应参数
    1. GPT3: 175 billion
    2. PaLM: 540 billion
2. 为什么小模型不行？
    1. 模型参数量大
    2. 和小模型使用相似的架构和预训练任务，但大模型能够解决更复杂的任务，小模型则不行
    3. 大模型可以学习上下文解决问题
3. LLM发展历程
    1. 1990: 统计学方法预测词汇 
    2. 2003： 采用深度学习
    3. 2018: Transformer问世
4. GPT的全称是？
    1. Generative pre-training model
5. 
