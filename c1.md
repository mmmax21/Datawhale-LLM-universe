# Data whale LLM universe

## 大模型简介
1. 举例大模型和相应参数
    1. GPT3: 175 billion
    2. PaLM: 540 billion
2. 为什么小模型不行？
    1. 模型参数量大
    2. 和小模型使用相似的架构和预训练任务，但大模型能够解决更复杂的任务，小模型则不行
    3. 大模型可以学习上下文解决问题
3. LLM发展历程
    1. 1990: 统计学方法预测词汇 
    2. 2003： 采用深度学习
    3. 2018: Transformer问世
4. GPT的全称是？
    1. Generative pre-training model
5. GPT4相比于GPT3.5有哪些提升？
    1. GPT4的参数量大约是GPT3.5的10倍，有更强的解决复杂问题的能力
    2. 将文本输入扩展到多模态
    3. GPT4 对恶意或挑衅性查询的响应更安全
6. LLM三个典型涌现能力
    1. 上下文学习：无需额外添加参数或更新就能理解上下文
    2. 指令遵循： 大模型能看懂自然语言描述，并根据指令完成任务，也是我们所说的微调
    3. 逐步推理： 小模型没办法解决涉及推理的复杂问题，但大模型可以通过CoT的方法解决
7. AGI是什么？
    1. 通用人工智能， 是和人类一样能思考和学习的智能

## RAG简介
1.大模型的缺点

| 大模型            | 问题                                                         | RAG解决方案                                                                                       |
|------------------|--------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
| 信息偏差/幻觉     | LLM有时会产生与客观事实不符的信息，例如给出不存在的网站。   | RAG通过检索数据源，让结果更可靠。                                                                |
| 知识更新滞后       | LLM训练的数据是静态的，比如说GPT3只用了2021年之前的数据训练。 | RAG使用实时检索，保持内容实效性。                                                                  |
| 内容不可追溯       | 内容缺乏信息来源。                                           | RAG给出生成内容的来源链接。                                                                      |
| 专业知识缺失       | 专业内容训练不够。                                           | RAG会检索专业知识文档。                                                                          |
| 推理能力不足       | 面对复杂问题时，LLM可能缺乏必要的推理能力。                | RAG提供额外的背景知识和数据支持。                                                                |
| 应用场景有限       | 难以用单一模型适应所有场景。                                | 通过检索对应应用场景数据的方式，灵活适应问答系统、推荐系统等多种应用场景。                         |
| 长文本能力较弱     | 受制于有限的上下文窗口。                                    | 检索和整合长文本信息，强化了模型对长上下文的理解和生成，有效突破了输入长度的限制。                 |


2. RAG的工作流程是什么？增强阶段做了什么？
    1. 收集-》 检索-〉 增强-》生成
    2. 增强就是对检索到的信息进行处理和增强，以便生成模型可以更好地理解和使用
3. RAG vs. 微调

|       技术         |                          RAG                           |                      微调                       |
|------------------|--------------------------------------------------------|--------------------------------------------------|
| 知识更新         | 无需重新训练，适合动态变化数据                            | 需要重新训练，适合静态数据                             |
| 数据处理         | 对数据要求低                                             | 依赖高质量数据集                                      |
| 模型定制         | No。侧重信息检索和融合，无法定制                          | Yes。可以根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识 |
| 可解释性         | 高，可以追溯到检索来源                                    | 低                                                    |
| 计算资源         | 高，需要搭建数据库和检索机制                               | 高                                                    |
| 隐私性           | 低。检索使用外部数据，有风险                               | 低。训练敏感数据需要额外处理                             |


## 开发LLM的整体流程
1. 传统AI开发 vs. 大模型开发
    1. 传统： 将整个业务拆解成N个子任务，每个子任务构造数据集并优化模型
    2. 大模型：构造小批量验证集，设计合理 Prompt 来满足验证集效果。收集bad case然后优化prompt，而不需要划分子任务。
2. 大模型开发一般流程

|     具体任务       |                        描述                                |
|------------------|-----------------------------------------------------------|
| 确定目标           | 构建最小可行性产品MVP并逐步优化                               |
| 设计功能           | 目标：个人知识库助手。核心功能：结合个人知识库内容进行问题的回答。上游功能：用户上传知识库。下游功能：用户手动纠正模型回答。 |
| 搭建整体架构       | 特定数据库 + prompt + 大模型                                |
| 搭建数据库         | 收集数据 -> 预处理 -> 向量化                                 |
| Prompt工程         | 明确prompt设计原则，构建小型验证集                           |
| 验证迭代           | 设计并使用bad case优化结果，应对边界情况                      |
| 前后端搭建         | Gradio/Streamlit                                           |
| 优化               | 根据反馈优化                                               |

3. 收集和处理数据时，需要注意什么？
    1. 由于大模型上下文窗口有限，我们需要对读取的文本进行切分，将较长的文本切分为较小的文本
4. 索引index创建指的是什么呢？
    1. 将文件切片、向量化后存入数据库。之后就可以用index找到每个对应的文档
5. 那么大模型的输入是什么？
    1. 用户的问题通过embedding进行向量化 -> 在chromaDB中找到最相似的文档 -》 检索到的文档作为上下文 + 用户的问题 -> 输入大模型

